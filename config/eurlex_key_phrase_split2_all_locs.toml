# default parameters
seed = 10
name = 'fixed_key_phrase_split2_all_locs_eurlex'
retain_ckp = true
[EXPERIMENT]
devices = 1
accelerator = 'gpu'
epochs = 100
accumulate_grad_batches = 1
save_path = 'results/experiments/fixed_key_phrase_split2_all_locs_eurlex/'
val_check_interval = 1.0
precision = 32
optimizer = 'adamw'
optimizer_params = {}
lrscheduler = 'cosinewarmup'
lrscheduler_params={}
lr = 0.00005
monitor = 'val_loss'
mode = 'min'
loss = 'fbce' # ce_1, ce
warmup = 2000
# steps = 3000
stop_strategy = 'early_stop'
  
[MODEL]
name = 'transformer_encoder'
dropout = 0.3
embedding  = {initialization='random', kwargs={dim=512,freeze=false}}
d_model = 512
hidden_dim = 2048
output_dim = 512
layers = 6
head = 8 
attn_mode = {name='key_phrase_split2',param1=10,param2='all_locs'}
pe_type = 'absolute_sin'

[DATA]
datasets_dir = 'data'
dataset_name = 'eurlex'
batch_size=64
train_ratio= 1
val_split_ratio = 0.1
num_workers = 4
max_seq_len = 512
tokenizer_type = "bert"
# tokenizer_name =  'wordpiece'
tokenizer_name = 'bert-base-uncased'
# tokenizer_name = 'roberta-base'
tokenizer_params = {clean=true,lowercase=true,add_spe_tokens=['[LOC]']}
processer = {name='eurlex'}







