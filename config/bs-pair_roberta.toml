# default parameters
seed = 10
name = 'bsp_roberta'
retain_ckp = true
# bs-pair_namekey_phrase_chunk_rep_param15_param2average_weights_param30.1_0.6_initializationchunk_pretrain_emb_model_nameroberta-base-uncased_dim768_freezeFalse_waysum_normTrue_bert_bert-base-uncased_.pt 0.5628

# average 5 0.1_0.7 0.5627
# average 5 0.1_0.8 0.5627
[EXPERIMENT]
devices = 1
accelerator = 'gpu'
epochs = 10
accumulate_grad_batches = 1
save_path = 'results/experiments/bsp_robert/'
val_check_interval = 1.0
precision = 32
optimizer = 'adamw'
optimizer_params = {}
lrscheduler = 'cosinewarmup'
lrscheduler_params={}
lr = 0.00005
monitor = 'val_loss'
mode = 'min'
loss = 'bce' # ce_1, ce
warmup = 0.1
# steps = 3000
stop_strategy = 'early_stop'
stop_patience = 10
  
[MODEL]
name = 'BERT'
model_name = 'roberta-base'
num_labels = 227
freeze = false
embedding  = {initialization='original'}
attn_mode = {name='default',param1=0}

[DATA]
datasets_dir = 'data'
dataset_name = 'bs-pair'
batch_size = 16
train_ratio = 1
val_split_ratio = 0.1
num_workers = 0
max_seq_len = 512
tokenizer_type = "bert"
# tokenizer_name = 'wordpiece'
tokenizer_name = 'roberta-base'
tokenizer_params = {clean=true,lowercase=true,add_spe_tokens=['[LOC]']}
processer = {name='bs'}







